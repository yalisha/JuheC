#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¤–éƒ¨APIé›†æˆæ¨¡å—
æ”¯æŒTwitterã€Redditã€YouTubeç­‰å›½å¤–å¹³å°æ•°æ®é‡‡é›†

å…è´¹APIé…é¢é™åˆ¶ï¼š
- Twitter Trends By Location: 100æ¬¡/æœˆ
- ReddAPI: 50æ¬¡/æœˆ
- Social Media Master: 70æ¬¡/æœˆ

è°ƒç”¨ç­–ç•¥ï¼šæ¯å¤©æœ€å¤šè°ƒç”¨1æ¬¡ï¼Œç¡®ä¿ä¸€ä¸ªæœˆä¸è¶…è¿‡é…é¢
"""

import http.client
import json
import logging
from typing import Dict, List, Optional
from datetime import datetime
from pathlib import Path


class ExternalAPICrawler:
    """å¤–éƒ¨APIçˆ¬è™«ç±»"""

    # RapidAPIå¯†é’¥
    RAPIDAPI_KEY = "e69e6c1b6dmsh73481792f82a139p13666ajsn3c0e61b0d6d3"

    def __init__(self):
        """åˆå§‹åŒ–å¤–éƒ¨APIçˆ¬è™«"""
        self.logger = logging.getLogger(__name__)

    def fetch_twitter_trends(self, location_id: str = "f719fcd7bc333af4b3d78d0e65893e5e") -> Optional[Dict]:
        """
        è·å–Twitterçƒ­é—¨è¶‹åŠ¿

        Args:
            location_id: åœ°åŒºIDï¼ˆé»˜è®¤ä¸ºç¾å›½ï¼‰
                - f719fcd7bc333af4b3d78d0e65893e5e: United States (ç¾å›½)
                - 1: Worldwide (å…¨çƒ)

        Returns:
            Twitterè¶‹åŠ¿æ•°æ®
        """
        try:
            conn = http.client.HTTPSConnection("twitter-trends-by-location.p.rapidapi.com")

            headers = {
                'x-rapidapi-key': self.RAPIDAPI_KEY,
                'x-rapidapi-host': "twitter-trends-by-location.p.rapidapi.com"
            }

            conn.request("GET", f"/location/{location_id}", headers=headers)

            res = conn.getresponse()
            data = res.read()

            result = json.loads(data.decode("utf-8"))

            # æ£€æŸ¥APIå“åº”çŠ¶æ€
            if result.get('status') != 'SUCCESS':
                self.logger.error(f"Twitter APIè¿”å›é”™è¯¯: {result.get('message')}")
                return None

            trending_data = result.get('trending', {})
            location_name = trending_data.get('name', 'Unknown')

            self.logger.info(f"æˆåŠŸè·å–Twitterè¶‹åŠ¿æ•°æ®: {location_name}")

            # è½¬æ¢ä¸ºç»Ÿä¸€æ ¼å¼
            trends = []
            for item in trending_data.get('trends', [])[:50]:  # å–å‰50ä¸ª
                trends.append({
                    'id': item.get('name', ''),
                    'title': item.get('name', ''),
                    'desc': f"Rank {item.get('rank', 0)}" + (f" | {item.get('domain', '')}" if item.get('domain') else ""),
                    'url': item.get('webUrl', ''),
                    'hot': item.get('postCount', 0) or 0,
                    'mobileUrl': item.get('mobileIntent', ''),
                    'rank': item.get('rank', 0),
                    'domain': item.get('domain', ''),
                    'timestamp': int(datetime.now().timestamp() * 1000)
                })

            return {
                'platform': 'Twitter',
                'location': location_name,
                'location_type': trending_data.get('locationType', 'Unknown'),
                'timestamp': datetime.now().isoformat(),
                'data': trends
            }

        except Exception as e:
            self.logger.error(f"è·å–Twitteræ•°æ®å¤±è´¥: {e}")
            import traceback
            self.logger.error(traceback.format_exc())
            return None
        finally:
            if 'conn' in locals():
                conn.close()

    def fetch_reddit_hot(self, subreddit: str = "popular", limit: int = 50) -> Optional[Dict]:
        """
        è·å–Redditçƒ­é—¨å¸–å­

        æ³¨æ„ï¼šéœ€è¦å…ˆåœ¨RapidAPIæ§åˆ¶å°æŸ¥çœ‹ReddAPIçš„æ­£ç¡®ç«¯ç‚¹
        å½“å‰ç«¯ç‚¹å¯èƒ½ä¸æ­£ç¡®ï¼Œéœ€è¦æ ¹æ®APIæ–‡æ¡£è°ƒæ•´

        Args:
            subreddit: å­ç‰ˆå—åç§°ï¼ˆé»˜è®¤popularï¼‰
            limit: è·å–æ•°é‡

        Returns:
            Redditçƒ­é—¨æ•°æ®
        """
        try:
            conn = http.client.HTTPSConnection("reddapi.p.rapidapi.com")

            headers = {
                'x-rapidapi-key': self.RAPIDAPI_KEY,
                'x-rapidapi-host': "reddapi.p.rapidapi.com"
            }

            # TODO: éœ€è¦æ ¹æ®RapidAPIæ–‡æ¡£æ›´æ–°æ­£ç¡®çš„ç«¯ç‚¹
            # ç›®å‰ä½¿ç”¨çš„ç«¯ç‚¹è¿”å›404ï¼Œéœ€è¦æŸ¥çœ‹APIæ–‡æ¡£
            endpoint = f"/hot?subreddit={subreddit}&limit={limit}"

            conn.request("GET", endpoint, headers=headers)

            res = conn.getresponse()
            data = res.read()

            if res.status != 200:
                self.logger.error(f"Reddit APIé”™è¯¯ {res.status}: {data.decode('utf-8')[:200]}")
                return None

            result = json.loads(data.decode("utf-8"))

            self.logger.info(f"æˆåŠŸè·å–Redditæ•°æ®")

            # è½¬æ¢ä¸ºç»Ÿä¸€æ ¼å¼ï¼ˆéœ€è¦æ ¹æ®å®é™…å“åº”è°ƒæ•´ï¼‰
            trends = []
            if isinstance(result, dict) and 'data' in result:
                for idx, item in enumerate(result['data'].get('children', [])[:limit], 1):
                    post = item.get('data', {})
                    trends.append({
                        'id': post.get('id', ''),
                        'title': post.get('title', ''),
                        'desc': post.get('selftext', '')[:200] if post.get('selftext') else '',
                        'author': post.get('author', ''),
                        'url': f"https://reddit.com{post.get('permalink', '')}",
                        'hot': post.get('score', 0) + post.get('num_comments', 0),
                        'score': post.get('score', 0),
                        'comments': post.get('num_comments', 0),
                        'timestamp': int(post.get('created_utc', 0) * 1000)
                    })

            return {
                'platform': 'Reddit',
                'subreddit': subreddit,
                'timestamp': datetime.now().isoformat(),
                'data': trends
            }

        except Exception as e:
            self.logger.error(f"è·å–Redditæ•°æ®å¤±è´¥: {e}")
            import traceback
            self.logger.error(traceback.format_exc())
            return None
        finally:
            if 'conn' in locals():
                conn.close()

    def fetch_youtube_trending(self, region: str = "US") -> Optional[Dict]:
        """
        è·å–YouTubeè¶‹åŠ¿è§†é¢‘

        æ³¨æ„ï¼šéœ€è¦å…ˆåœ¨RapidAPIæ§åˆ¶å°æŸ¥çœ‹Social Media Masterçš„æ­£ç¡®ç«¯ç‚¹
        å½“å‰ç«¯ç‚¹å¯èƒ½ä¸æ­£ç¡®ï¼Œéœ€è¦æ ¹æ®APIæ–‡æ¡£è°ƒæ•´

        Args:
            region: åœ°åŒºä»£ç ï¼ˆUS, GB, JPç­‰ï¼‰

        Returns:
            YouTubeè¶‹åŠ¿æ•°æ®
        """
        try:
            conn = http.client.HTTPSConnection("social-media-master.p.rapidapi.com")

            headers = {
                'x-rapidapi-key': self.RAPIDAPI_KEY,
                'x-rapidapi-host': "social-media-master.p.rapidapi.com"
            }

            # TODO: éœ€è¦æ ¹æ®RapidAPIæ–‡æ¡£æ›´æ–°æ­£ç¡®çš„ç«¯ç‚¹
            # ç›®å‰ä½¿ç”¨çš„ç«¯ç‚¹è¿”å›404ï¼Œéœ€è¦æŸ¥çœ‹APIæ–‡æ¡£
            endpoint = f"/trending?platform=youtube&region={region}"

            conn.request("GET", endpoint, headers=headers)

            res = conn.getresponse()
            data = res.read()

            if res.status != 200:
                self.logger.error(f"YouTube APIé”™è¯¯ {res.status}: {data.decode('utf-8')[:200]}")
                return None

            result = json.loads(data.decode("utf-8"))

            self.logger.info(f"æˆåŠŸè·å–YouTubeæ•°æ®")

            # è½¬æ¢ä¸ºç»Ÿä¸€æ ¼å¼ï¼ˆéœ€è¦æ ¹æ®å®é™…å“åº”è°ƒæ•´ï¼‰
            trends = []
            if isinstance(result, list):
                for item in result[:50]:
                    trends.append({
                        'id': item.get('id', ''),
                        'title': item.get('title', ''),
                        'desc': item.get('description', '')[:200] if item.get('description') else '',
                        'author': item.get('channel_title', ''),
                        'url': f"https://youtube.com/watch?v={item.get('id', '')}",
                        'hot': item.get('view_count', 0),
                        'views': item.get('view_count', 0),
                        'likes': item.get('like_count', 0),
                        'timestamp': int(datetime.now().timestamp() * 1000),
                        'cover': item.get('thumbnail', '')
                    })

            return {
                'platform': 'YouTube',
                'region': region,
                'timestamp': datetime.now().isoformat(),
                'data': trends
            }

        except Exception as e:
            self.logger.error(f"è·å–YouTubeæ•°æ®å¤±è´¥: {e}")
            import traceback
            self.logger.error(traceback.format_exc())
            return None
        finally:
            if 'conn' in locals():
                conn.close()

    def fetch_all_external(self) -> Dict:
        """
        è·å–æ‰€æœ‰å¤–éƒ¨å¹³å°æ•°æ®

        æ³¨æ„ï¼šæ­¤å‡½æ•°ä¼šæ¶ˆè€—APIé…é¢ï¼Œå»ºè®®æ¯å¤©æœ€å¤šè°ƒç”¨1æ¬¡

        Returns:
            æ‰€æœ‰å¤–éƒ¨å¹³å°æ•°æ®
        """
        results = {
            'crawl_time': datetime.now().isoformat(),
            'platforms': {}
        }

        # Twitter - å”¯ä¸€æ­£å¸¸å·¥ä½œçš„API
        twitter_data = self.fetch_twitter_trends()
        if twitter_data:
            results['platforms']['Twitter'] = twitter_data

        # Reddit - ç«¯ç‚¹éœ€è¦ä¿®å¤
        # æš‚æ—¶æ³¨é‡Šæ‰ï¼Œéœ€è¦åœ¨RapidAPIæ§åˆ¶å°æŸ¥çœ‹æ­£ç¡®çš„ç«¯ç‚¹
        # reddit_data = self.fetch_reddit_hot()
        # if reddit_data:
        #     results['platforms']['Reddit'] = reddit_data

        # YouTube - ç«¯ç‚¹éœ€è¦ä¿®å¤
        # æš‚æ—¶æ³¨é‡Šæ‰ï¼Œéœ€è¦åœ¨RapidAPIæ§åˆ¶å°æŸ¥çœ‹æ­£ç¡®çš„ç«¯ç‚¹
        # youtube_data = self.fetch_youtube_trending()
        # if youtube_data:
        #     results['platforms']['YouTube'] = youtube_data

        self.logger.info(f"å¤–éƒ¨APIçˆ¬å–å®Œæˆï¼ŒæˆåŠŸ {len(results['platforms'])} ä¸ªå¹³å°")

        return results


if __name__ == "__main__":
    # æµ‹è¯•ä»£ç 
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )

    crawler = ExternalAPICrawler()

    # æµ‹è¯•Twitter
    print("\n" + "=" * 60)
    print("æµ‹è¯•Twitter API")
    print("=" * 60)
    twitter_data = crawler.fetch_twitter_trends()
    if twitter_data:
        print(f"âœ… è·å–åˆ° {len(twitter_data['data'])} æ¡Twitterè¶‹åŠ¿")
        print(f"ğŸ“ åœ°åŒº: {twitter_data['location']} ({twitter_data['location_type']})")
        if twitter_data['data']:
            print(f"\nå‰3æ¡ç¤ºä¾‹:")
            for i, item in enumerate(twitter_data['data'][:3], 1):
                print(f"{i}. {item['title']} - çƒ­åº¦:{item['hot']} - {item['desc']}")
    else:
        print("âŒ Twitter APIè°ƒç”¨å¤±è´¥")

    # æµ‹è¯•æ‰€æœ‰
    print("\n" + "=" * 60)
    print("æµ‹è¯•æ‰€æœ‰å¤–éƒ¨API")
    print("=" * 60)
    all_data = crawler.fetch_all_external()
    print(f"æ€»å…±è·å– {len(all_data['platforms'])} ä¸ªå¹³å°æ•°æ®")
    for platform, data in all_data['platforms'].items():
        print(f"  - {platform}: {len(data['data'])} æ¡æ•°æ®")
