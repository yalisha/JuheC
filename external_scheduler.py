#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¤–éƒ¨APIè°ƒåº¦å™¨
é™åˆ¶è°ƒç”¨é¢‘ç‡ï¼šæ¯å¤©æœ€å¤š1æ¬¡ï¼Œç¡®ä¿ä¸è¶…è¿‡æœˆåº¦å…è´¹é…é¢

æœˆåº¦é…é¢:
- Twitter Trends: 100æ¬¡/æœˆ
- Reddit API: 50æ¬¡/æœˆ
- YouTube API: 70æ¬¡/æœˆ

æ¯å¤©1æ¬¡è°ƒç”¨ = 30æ¬¡/æœˆ < æœ€ä½é…é¢(50æ¬¡)
"""

import schedule
import time
import logging
import json
from pathlib import Path
from datetime import datetime, timedelta
from external_apis import ExternalAPICrawler


class ExternalAPIScheduler:
    """å¤–éƒ¨APIè°ƒåº¦å™¨ - æ¯å¤©è°ƒç”¨1æ¬¡"""

    def __init__(self, data_dir: str = "data", log_dir: str = "logs"):
        """
        åˆå§‹åŒ–è°ƒåº¦å™¨

        Args:
            data_dir: æ•°æ®ç›®å½•
            log_dir: æ—¥å¿—ç›®å½•
        """
        self.data_dir = Path(data_dir)
        self.log_dir = Path(log_dir)
        self.data_dir.mkdir(exist_ok=True)
        self.log_dir.mkdir(exist_ok=True)

        # è®¾ç½®æ—¥å¿—
        log_file = self.log_dir / "external_api.log"
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler(log_file, encoding='utf-8'),
                logging.StreamHandler()
            ]
        )
        self.logger = logging.getLogger(__name__)

        # çŠ¶æ€æ–‡ä»¶ - è®°å½•æœ€åä¸€æ¬¡è°ƒç”¨æ—¶é—´
        self.status_file = self.data_dir / "external_api_status.json"

        # åˆå§‹åŒ–çˆ¬è™«
        self.crawler = ExternalAPICrawler()

    def can_crawl_today(self) -> bool:
        """
        æ£€æŸ¥ä»Šå¤©æ˜¯å¦å·²ç»è°ƒç”¨è¿‡API

        Returns:
            True if ä»Šå¤©è¿˜æ²¡è°ƒç”¨è¿‡, False if ä»Šå¤©å·²ç»è°ƒç”¨è¿‡
        """
        if not self.status_file.exists():
            return True

        try:
            with open(self.status_file, 'r', encoding='utf-8') as f:
                status = json.load(f)

            last_crawl = datetime.fromisoformat(status.get('last_crawl_time', '2000-01-01'))
            today = datetime.now().date()

            # å¦‚æœæœ€åä¸€æ¬¡è°ƒç”¨ä¸æ˜¯ä»Šå¤©ï¼Œåˆ™å¯ä»¥è°ƒç”¨
            return last_crawl.date() < today

        except Exception as e:
            self.logger.error(f"è¯»å–çŠ¶æ€æ–‡ä»¶å¤±è´¥: {e}")
            return True

    def update_status(self):
        """æ›´æ–°çŠ¶æ€æ–‡ä»¶"""
        status = {
            'last_crawl_time': datetime.now().isoformat(),
            'total_calls_this_month': self._get_monthly_calls() + 1
        }

        try:
            with open(self.status_file, 'w', encoding='utf-8') as f:
                json.dump(status, f, ensure_ascii=False, indent=2)
        except Exception as e:
            self.logger.error(f"æ›´æ–°çŠ¶æ€æ–‡ä»¶å¤±è´¥: {e}")

    def _get_monthly_calls(self) -> int:
        """è·å–æœ¬æœˆå·²è°ƒç”¨æ¬¡æ•°"""
        if not self.status_file.exists():
            return 0

        try:
            with open(self.status_file, 'r', encoding='utf-8') as f:
                status = json.load(f)

            last_crawl = datetime.fromisoformat(status.get('last_crawl_time', '2000-01-01'))
            current_month = datetime.now().month

            # å¦‚æœæ˜¯åŒä¸€ä¸ªæœˆï¼Œè¿”å›æ¬¡æ•°ï¼›å¦åˆ™é‡ç½®ä¸º0
            if last_crawl.month == current_month:
                return status.get('total_calls_this_month', 0)
            else:
                return 0

        except Exception as e:
            self.logger.error(f"è¯»å–æœˆåº¦è°ƒç”¨æ¬¡æ•°å¤±è´¥: {e}")
            return 0

    def job(self):
        """æ‰§è¡Œå¤–éƒ¨APIçˆ¬å–ä»»åŠ¡"""
        # æ£€æŸ¥ä»Šå¤©æ˜¯å¦å·²ç»è°ƒç”¨è¿‡
        if not self.can_crawl_today():
            self.logger.info("â­ï¸  ä»Šå¤©å·²ç»è°ƒç”¨è¿‡å¤–éƒ¨APIï¼Œè·³è¿‡æœ¬æ¬¡è°ƒç”¨")
            return

        self.logger.info("ğŸŒ å¼€å§‹å¤–éƒ¨APIçˆ¬å–...")

        try:
            # è°ƒç”¨API
            results = self.crawler.fetch_all_external()

            # ä¿å­˜æ•°æ®
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            output_file = self.data_dir / f"external_{timestamp}.json"

            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(results, f, ensure_ascii=False, indent=2)

            # ç»Ÿè®¡ä¿¡æ¯
            total_items = sum(len(p['data']) for p in results['platforms'].values())
            monthly_calls = self._get_monthly_calls() + 1

            self.logger.info(f"âœ… å¤–éƒ¨APIçˆ¬å–å®Œæˆ")
            self.logger.info(f"   å¹³å°æ•°: {len(results['platforms'])}")
            self.logger.info(f"   æ€»æ¡ç›®: {total_items}")
            self.logger.info(f"   æœ¬æœˆè°ƒç”¨: {monthly_calls} æ¬¡")
            self.logger.info(f"   ä¿å­˜åˆ°: {output_file}")

            # æ›´æ–°çŠ¶æ€
            self.update_status()

        except Exception as e:
            self.logger.error(f"âŒ å¤–éƒ¨APIçˆ¬å–å¤±è´¥: {e}")
            import traceback
            self.logger.error(traceback.format_exc())

    def run(self, daily_hour: int = 2):
        """
        è¿è¡Œè°ƒåº¦å™¨

        Args:
            daily_hour: æ¯å¤©æ‰§è¡Œçš„å°æ—¶ï¼ˆé»˜è®¤å‡Œæ™¨2ç‚¹ï¼‰
        """
        self.logger.info(f"ğŸš€ å¤–éƒ¨APIè°ƒåº¦å™¨å¯åŠ¨")
        self.logger.info(f"â° æ‰§è¡Œæ—¶é—´: æ¯å¤© {daily_hour:02d}:00")
        self.logger.info(f"ğŸ“Š APIé…é¢é™åˆ¶:")
        self.logger.info(f"   - Twitter: 100æ¬¡/æœˆ")
        self.logger.info(f"   - Reddit: 50æ¬¡/æœˆ")
        self.logger.info(f"   - YouTube: 70æ¬¡/æœˆ")
        self.logger.info(f"   - è°ƒåº¦ç­–ç•¥: æ¯å¤©1æ¬¡ = 30æ¬¡/æœˆ âœ…")

        # è®¾ç½®å®šæ—¶ä»»åŠ¡ - æ¯å¤©æŒ‡å®šæ—¶é—´æ‰§è¡Œ
        schedule.every().day.at(f"{daily_hour:02d}:00").do(self.job)

        # ç«‹å³æ‰§è¡Œä¸€æ¬¡ï¼ˆå¦‚æœä»Šå¤©è¿˜æ²¡æ‰§è¡Œè¿‡ï¼‰
        self.job()

        # è¿›å…¥è°ƒåº¦å¾ªç¯
        while True:
            schedule.run_pending()
            time.sleep(60)  # æ¯åˆ†é’Ÿæ£€æŸ¥ä¸€æ¬¡


if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser(description="å¤–éƒ¨APIè°ƒåº¦å™¨ - æ¯å¤©è°ƒç”¨1æ¬¡")
    parser.add_argument('--hour', type=int, default=2,
                        help='æ¯å¤©æ‰§è¡Œçš„å°æ—¶ (0-23)ï¼Œé»˜è®¤ä¸º2ï¼ˆå‡Œæ™¨2ç‚¹ï¼‰')
    parser.add_argument('--once', action='store_true',
                        help='åªæ‰§è¡Œä¸€æ¬¡ï¼ˆæµ‹è¯•ç”¨ï¼‰')

    args = parser.parse_args()

    scheduler = ExternalAPIScheduler()

    if args.once:
        # æµ‹è¯•æ¨¡å¼ - åªæ‰§è¡Œä¸€æ¬¡
        print("ğŸ“ æµ‹è¯•æ¨¡å¼ - åªæ‰§è¡Œä¸€æ¬¡")
        scheduler.job()
    else:
        # æ­£å¸¸æ¨¡å¼ - å®šæ—¶è°ƒåº¦
        scheduler.run(daily_hour=args.hour)
