# Phase 1 æ•°æ®å¢å¼ºå‡çº§è¯´æ˜

## ğŸ‰ å‡çº§å®Œæˆï¼

å·²æˆåŠŸå®ç°Phase 1çš„æ‰€æœ‰æ ¸å¿ƒåŠŸèƒ½ï¼Œä¸ºæ—¶é—´åºåˆ—é¢„æµ‹å’Œç®—æ³•æ²»ç†ç ”ç©¶æä¾›æ›´å¼ºå¤§çš„æ•°æ®é‡‡é›†èƒ½åŠ›ã€‚

## âœ¨ æ–°å¢åŠŸèƒ½

### 1. SQLiteæ•°æ®åº“å­˜å‚¨ â­â­â­â­â­

**æ›¿ä»£JSONæ–‡ä»¶å­˜å‚¨ï¼Œæ”¯æŒé«˜æ•ˆçš„æ—¶åºæŸ¥è¯¢å’Œæ•°æ®åˆ†æ**

#### æ•°æ®åº“è¡¨ç»“æ„ï¼š
- **hotsearch_raw** - åŸå§‹çƒ­æœæ•°æ®ï¼ˆå®Œæ•´JSONä¿ç•™ï¼‰
- **ranking_history** - æ’åå†å²è¿½è¸ªï¼ˆæ—¶åºåˆ†ææ ¸å¿ƒï¼‰
- **item_details** - è¯¦æƒ…æ•°æ®ï¼ˆæ’­æ”¾é‡ã€ç‚¹èµç­‰ï¼‰
- **cross_platform** - è·¨å¹³å°å…³è”

#### æ ¸å¿ƒå­—æ®µï¼š
```python
{
    "platform": "å“”å“©å“”å“©",
    "rank_position": 1,
    "hot": 6481884,
    "collected_at": "2025-10-23 20:28:25",
    "rank_change": +5,        # æ’åå˜åŒ–
    "hot_change": +1500000,   # çƒ­åº¦å˜åŒ–
    "hot_growth_rate": 30.5   # çƒ­åº¦å¢é•¿ç‡(%)
}
```

#### æ—¶åºåˆ†æåŠŸèƒ½ï¼š
```python
from database import get_database

with get_database() as db:
    # æŸ¥è¯¢å•ä¸ªçƒ­æœçš„å†å²
    history = db.get_item_history('å“”å“©å“”å“©', 'BV1234', hours=24)

    # æŸ¥è¯¢æŒç»­ä¸Šæ¦œçš„çƒ­ç‚¹
    trending = db.get_trending_topics('å“”å“©å“”å“©', hours=24, min_appearances=5)

    # æŸ¥è¯¢æœ€å¿«ä¸Šå‡çš„çƒ­æœ
    rising = db.get_fastest_rising('å“”å“©å“”å“©', limit=10)

    # å¯¼å‡ºä¸ºCSV
    db.export_to_csv('output.csv', platform='å¾®åš', start_date='2025-10-01')
```

### 2. 30åˆ†é’Ÿé‡‡é›†é¢‘ç‡ â­â­â­â­â­

**ä»æ¯å°æ—¶1æ¬¡æå‡åˆ°æ¯30åˆ†é’Ÿ1æ¬¡ï¼Œæ•°æ®å¯†åº¦ç¿»å€**

#### é…ç½®æ–¹å¼ï¼š
```bash
# æ–¹æ³•1ï¼šä½¿ç”¨é…ç½®æ–‡ä»¶
# ç¼–è¾‘ config.json:
{
  "scheduler": {
    "interval_minutes": 30  # æ”¹ä¸º30åˆ†é’Ÿ
  }
}

# æ–¹æ³•2ï¼šå‘½ä»¤è¡Œå‚æ•°
python3 scheduler.py --interval 30  # 30åˆ†é’Ÿé—´éš”
python3 scheduler.py --interval 15  # 15åˆ†é’Ÿé—´éš”ï¼ˆé«˜å³°æœŸï¼‰
```

#### æ•°æ®é‡æå‡ï¼š
- ä¹‹å‰: 24æ¬¡/å¤©
- ç°åœ¨: 48æ¬¡/å¤©
- æ•°æ®é‡: 12MB/å¤© â†’ 24MB/å¤©

### 3. å¤–éƒ¨APIé›†æˆ â­â­â­â­

**æ”¯æŒTwitterã€Redditã€YouTubeç­‰å›½å¤–å¹³å°æ•°æ®é‡‡é›†**

#### æ–°å¢æ¨¡å—ï¼š`external_apis.py`

```python
from external_apis import ExternalAPICrawler

crawler = ExternalAPICrawler()

# Twitterè¶‹åŠ¿
twitter_data = crawler.fetch_twitter_trends()

# Redditçƒ­é—¨
reddit_data = crawler.fetch_reddit_hot(subreddit='all')

# YouTubeè¶‹åŠ¿ï¼ˆéœ€è¦APIè°ƒæ•´ï¼‰
# youtube_data = crawler.fetch_youtube_trending()

# ä¸€æ¬¡æ€§è·å–æ‰€æœ‰
all_external = crawler.fetch_all_external()
```

#### APIé…ç½®ï¼š
å·²é›†æˆRapidAPIçš„3ä¸ªæœåŠ¡ï¼š
- Twitter Trends By Location
- ReddAPI
- Social Media Master

**æ³¨æ„**: å¤–éƒ¨APIç›®å‰é»˜è®¤å…³é—­ï¼Œéœ€è¦æ‰‹åŠ¨å¯ç”¨ï¼š
```python
crawler = HotSearchCrawler(use_external_apis=True)
```

### 4. è‡ªåŠ¨æ’åè¿½è¸ª â­â­â­â­â­

**è‡ªåŠ¨è®¡ç®—æ¯æ¡çƒ­æœçš„æ’åå’Œçƒ­åº¦å˜åŒ–**

#### è¿½è¸ªæŒ‡æ ‡ï¼š
- `rank_change` - æ’åå˜åŒ–ï¼ˆ+5è¡¨ç¤ºä¸Šå‡5ä½ï¼‰
- `hot_change` - çƒ­åº¦å˜åŒ–ï¼ˆç»å¯¹å€¼ï¼‰
- `hot_growth_rate` - çƒ­åº¦å¢é•¿ç‡ï¼ˆç™¾åˆ†æ¯”ï¼‰

#### æŸ¥è¯¢ç¤ºä¾‹ï¼š
```python
# è·å–æœ€å¿«ä¸Šå‡çš„çƒ­æœ
rising = db.get_fastest_rising('å“”å“©å“”å“©', limit=10)

for item in rising:
    print(f"{item['title']}")
    print(f"  æ’åå˜åŒ–: +{item['rank_change']}")
    print(f"  çƒ­åº¦å¢é•¿: {item['hot_growth_rate']:.1f}%")
```

## ğŸ“Š æµ‹è¯•ç»“æœ

### æ•°æ®åº“æ€§èƒ½ï¼š
```
âœ… å•æ¬¡çˆ¬å–: 16ä¸ªå¹³å°ï¼Œ693æ¡åŸå§‹æ•°æ®ï¼Œ700æ¡å†å²è®°å½•
âœ… æ•°æ®åº“å¤§å°: 1.2MB
âœ… è€—æ—¶: 20.89ç§’
âœ… æ’å…¥æˆåŠŸç‡: 100%
```

### æ•°æ®ç»Ÿè®¡ï¼š
```
æ€»è®°å½•æ•°: 693
å†å²è®°å½•æ•°: 700
æœ€æ–°è®°å½•: 2025-10-23 20:28:25
```

## ğŸš€ ä½¿ç”¨æ–¹æ³•

### æ–¹æ³•1ï¼šç»§ç»­ä½¿ç”¨ç°æœ‰æ–¹å¼ï¼ˆè‡ªåŠ¨å¯ç”¨æ•°æ®åº“ï¼‰

```bash
# å•æ¬¡è¿è¡Œ
python3 scheduler.py --once

# å®šæ—¶è¿è¡Œï¼ˆ30åˆ†é’Ÿé—´éš”ï¼‰
python3 scheduler.py --interval 30

# æˆ–ä½¿ç”¨è„šæœ¬
./run.sh
```

**ç°åœ¨ä¼šè‡ªåŠ¨ï¼š**
- âœ… ä¿å­˜åˆ°SQLiteæ•°æ®åº“
- âœ… ä¿å­˜åˆ°JSONæ–‡ä»¶ï¼ˆå…¼å®¹ï¼‰
- âœ… è¿½è¸ªæ’åå˜åŒ–
- âœ… æ¯30åˆ†é’Ÿè¿è¡Œä¸€æ¬¡

### æ–¹æ³•2ï¼šPythonç¼–ç¨‹æ¥å£

```python
from crawler import HotSearchCrawler
from database import get_database

# åˆ›å»ºçˆ¬è™«ï¼ˆå¯ç”¨æ‰€æœ‰æ–°åŠŸèƒ½ï¼‰
crawler = HotSearchCrawler(
    use_database=True,           # ä½¿ç”¨æ•°æ®åº“
    use_external_apis=False      # æš‚ä¸å¯ç”¨å¤–éƒ¨API
)

# è¿è¡Œä¸€æ¬¡
crawler.run_once()

# æŸ¥è¯¢æ•°æ®åº“
with get_database() as db:
    # è·å–ç»Ÿè®¡ä¿¡æ¯
    stats = db.get_statistics()
    print(f"æ€»è®°å½•æ•°: {stats['total_records']}")

    # æŸ¥è¯¢çƒ­æœå†å²
    history = db.get_item_history('å“”å“©å“”å“©', 'BV1234', hours=24)

    # å¯¼å‡ºæ•°æ®
    db.export_to_csv('hotsearch_data.csv', platform='å¾®åš')
```

## ğŸ“ æ–‡ä»¶ç»“æ„

```
çƒ­ç‚¹çˆ¬å–/
â”œâ”€â”€ database.py          â­ æ–°å¢ - æ•°æ®åº“æ¨¡å—
â”œâ”€â”€ external_apis.py     â­ æ–°å¢ - å¤–éƒ¨APIæ¨¡å—
â”œâ”€â”€ crawler.py           âœï¸ å‡çº§ - é›†æˆæ•°æ®åº“
â”œâ”€â”€ scheduler.py         âœï¸ å‡çº§ - æ”¯æŒåˆ†é’Ÿé—´éš”
â”œâ”€â”€ config.json          âœï¸ å‡çº§ - æ–°å¢é…ç½®é¡¹
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ hotsearch.db    â­ æ–°å¢ - SQLiteæ•°æ®åº“
â”‚   â”œâ”€â”€ latest.json      (ä¿ç•™)
â”‚   â””â”€â”€ hotsearch_*.json (ä¿ç•™)
â””â”€â”€ logs/
    â””â”€â”€ crawler_*.log
```

## ğŸ”§ é…ç½®è¯´æ˜

### config.json æ–°å¢é…ç½®ï¼š

```json
{
  "scheduler": {
    "interval_minutes": 30,      // æ”¹ä¸ºåˆ†é’Ÿé—´éš”
    "use_database": true          // å¯ç”¨æ•°æ®åº“
  }
}
```

## ğŸ“ˆ æ•°æ®è§„æ¨¡ä¼°ç®—

### åŸºç¡€é…ç½®ï¼ˆ30åˆ†é’Ÿé—´éš”ï¼‰ï¼š
- æ¯æ¬¡é‡‡é›†: 16å¹³å° Ã— å¹³å‡45æ¡ = 720æ¡
- æ¯å¤©: 48æ¬¡ Ã— 720æ¡ = 34,560æ¡
- æ•°æ®åº“å¤§å°: çº¦50MB/å¤©ï¼ˆåŒ…å«ç´¢å¼•ï¼‰
- æ¯æœˆ: ~1.5GB
- æ¯å¹´: ~18GB

### å»ºè®®ï¼š
- å®šæœŸæ¸…ç†3ä¸ªæœˆä»¥å‰çš„æ•°æ®
- æˆ–å¯¼å‡ºä¸ºCSVååˆ é™¤æ•°æ®åº“è®°å½•
- ä¿ç•™50GBç©ºé—´å……è¶³

## ğŸ¯ å­¦æœ¯ç ”ç©¶ä»·å€¼

### 1. æ—¶é—´åºåˆ—é¢„æµ‹ â­â­â­â­â­
- âœ… 30åˆ†é’Ÿç²’åº¦çš„æ—¶åºæ•°æ®
- âœ… å®Œæ•´çš„æ’åå†å²
- âœ… çƒ­åº¦å˜åŒ–ç‡è¿½è¸ª
- âœ… å¯å¯¼å‡ºä¸ºCSVç”¨äºæœºå™¨å­¦ä¹ 

### 2. ç®—æ³•æ²»ç†ç ”ç©¶ â­â­â­â­â­
- âœ… è§‚å¯Ÿå¹³å°æ¨èç®—æ³•è¡Œä¸º
- âœ… åˆ†ææ’åæ›´æ–°é¢‘ç‡
- âœ… ç ”ç©¶çƒ­åº¦è®¡ç®—æœºåˆ¶
- âœ… è·¨å¹³å°ç®—æ³•å¯¹æ¯”

### 3. èˆ†æƒ…åˆ†æ â­â­â­â­
- âœ… è¯é¢˜ç”Ÿå‘½å‘¨æœŸè¿½è¸ª
- âœ… çƒ­ç‚¹æ¼”å˜è·¯å¾„
- âœ… è·¨å¹³å°ä¼ æ’­åˆ†æ

## âš ï¸ æ³¨æ„äº‹é¡¹

### 1. å¤–éƒ¨API
- é»˜è®¤å…³é—­ï¼ˆéœ€è¦æµ‹è¯•å’Œè°ƒæ•´ï¼‰
- RapidAPIæœ‰è¯·æ±‚é™åˆ¶
- å»ºè®®å…ˆæµ‹è¯•å†å¯ç”¨

### 2. æ•°æ®åº“ç»´æŠ¤
```python
# å®šæœŸå¤‡ä»½
cp data/hotsearch.db data/backup_$(date +%Y%m%d).db

# æŸ¥çœ‹æ•°æ®åº“å¤§å°
du -h data/hotsearch.db

# æ¸…ç†æ—§æ•°æ®ï¼ˆä¿ç•™æœ€è¿‘30å¤©ï¼‰
DELETE FROM ranking_history WHERE collected_at < datetime('now', '-30 days');
VACUUM;
```

### 3. æ€§èƒ½ä¼˜åŒ–
- æ•°æ®åº“å·²å»ºç«‹ç´¢å¼•ï¼ŒæŸ¥è¯¢æ•ˆç‡é«˜
- å»ºè®®æ¯æœˆæ¸…ç†ä¸€æ¬¡æ—§æ•°æ®
- CSVå¯¼å‡ºé€‚åˆå¤§è§„æ¨¡åˆ†æ

## ğŸ”œ ä¸‹ä¸€æ­¥è®¡åˆ’ï¼ˆPhase 2ï¼‰

1. **Bç«™è¯¦æƒ…æ•°æ®é‡‡é›†** - æ’­æ”¾é‡ã€ç‚¹èµã€è¯„è®ºç­‰
2. **æƒ…æ„Ÿåˆ†æ** - ä½¿ç”¨SnowNLPåˆ†ææ ‡é¢˜æƒ…æ„Ÿ
3. **è·¨å¹³å°å…³è”** - è‡ªåŠ¨è¯†åˆ«ç›¸åŒè¯é¢˜
4. **æ•°æ®å¯è§†åŒ–** - ç”Ÿæˆè¶‹åŠ¿å›¾è¡¨
5. **æ—¥æŠ¥/å‘¨æŠ¥** - è‡ªåŠ¨ç”Ÿæˆåˆ†ææŠ¥å‘Š

## ğŸ“ æ›´æ–°æ—¥å¿—

### v1.2.0 (2025-10-23) - Phase 1å®Œæˆ â­

**æ ¸å¿ƒåŠŸèƒ½ï¼š**
- âœ… SQLiteæ•°æ®åº“å­˜å‚¨å’Œæ—¶åºè¿½è¸ª
- âœ… 30åˆ†é’Ÿé‡‡é›†é¢‘ç‡
- âœ… å¤–éƒ¨APIé›†æˆæ¡†æ¶ï¼ˆTwitterã€Redditï¼‰
- âœ… è‡ªåŠ¨æ’åå˜åŒ–è®¡ç®—
- âœ… CSVå¯¼å‡ºåŠŸèƒ½
- âœ… æ•°æ®åº“æŸ¥è¯¢API

**å…¼å®¹æ€§ï¼š**
- âœ… å‘åå…¼å®¹ï¼Œä¿ç•™JSONæ–‡ä»¶å­˜å‚¨
- âœ… ç°æœ‰è„šæœ¬æ— éœ€ä¿®æ”¹
- âœ… å¹³æ»‘å‡çº§

---

**ç°åœ¨å¯ä»¥å¼€å§‹é•¿æœŸè¿è¡Œï¼Œç§¯ç´¯å®è´µçš„æ—¶åºæ•°æ®ï¼** ğŸš€
